{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "\n",
    "Download zip files from github and unzip into mounted location. Once unzipped, read data files and load into dataframes.\n",
    "Plot histogram to show label occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if extracted data folder exists\n",
    "usingColab = False\n",
    "if usingColab:\n",
    "  if(not os.path.exists('SYSC4415W23_A3_dataset')):\n",
    "\n",
    "    # Download and extract the dataset if the zip file does not exist\n",
    "    if (not os.path.isfile('SYSC4415W23_A3_dataset.zip')):\n",
    "      !wget https://github.com/jrgreen7/SYSC4906/releases/download/Assignment3/SYSC4415W23_A3_dataset.zip\n",
    "      !unzip SYSC4415W23_A3_dataset.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Training Data\n",
    "train_labels = pd.read_csv(f'SYSC4415W23_A3_dataset/train/labels.csv')\n",
    "train_features = pd.read_csv(f'SYSC4415W23_A3_dataset/train/extracted_features.csv')\n",
    "\n",
    "train_features.set_index(keys='sample_id', inplace=True)\n",
    "train_labels.set_index(keys='sample_id', inplace=True)\n",
    "\n",
    "train_features.sort_values(by=['sample_id'], inplace=True)\n",
    "train_labels.sort_values(by=['sample_id'], inplace=True)\n",
    "\n",
    "print(f'Shape of original features: {train_features.shape}')\n",
    "\n",
    "# Evaluation Test Data\n",
    "test_features = pd.read_csv(f'SYSC4415W23_A3_dataset/test/extracted_features.csv')\n",
    "test_labels = pd.read_csv(f'SYSC4415W23_A3_dataset/test/labels.csv')\n",
    "\n",
    "test_features.set_index('sample_id', inplace=True)\n",
    "test_labels.set_index('sample_id', inplace=True)\n",
    "\n",
    "test_features.sort_values(by=['sample_id'], inplace=True)\n",
    "test_labels.sort_values(by=['sample_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data visualization\n",
    "counts = train_labels.value_counts(subset=['label'])\n",
    "keys = ['Normal Walk', 'Fast Walk', 'Ascent', 'Descent', 'Jumping Jacks']\n",
    "vals = [counts[0],counts[1], counts[2], counts[3], counts[4]]\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.bar(x=keys, height=vals, width=0.5)\n",
    "plt.xlabel(\"Data Label\")\n",
    "plt.ylabel(\"Num samples\")\n",
    "plt.title(\"Number of each type of training sample\")\n",
    "plt.show()\n",
    "\n",
    "# No class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection \n",
    "Using variance thresholding, dropping any features with NaN values and using univariate stats to determince the ***__top 50% of features__*** for classification to determine which features are useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features that have very low variance\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif\n",
    "\n",
    "# Feature Selection, done as a function to accomodate different features and methods needed by different models\n",
    "def trimDataset(numFeatures: int, useCorr=False) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "\n",
    "    # Drop columns with nan values\n",
    "    toDrop = train_features.columns[train_features.isnull().any()].tolist()\n",
    "    train_selected = train_features.drop(toDrop, axis=1)\n",
    "    test_selected = test_features.drop(toDrop, axis=1)\n",
    "\n",
    "    # Drop columns with very low variance\n",
    "    sel = VarianceThreshold(threshold=(0.95 * (1 - 0.95)))\n",
    "    sel.fit_transform(train_selected)\n",
    "    cols = [column for column in train_selected.columns \n",
    "            if column not in train_selected.columns[sel.get_support()]]\n",
    "    train_selected.drop(columns=cols, inplace=True)\n",
    "    test_selected.drop(columns=cols, inplace=True)\n",
    "\n",
    "    # Compute the correlation matrix\n",
    "    if useCorr:\n",
    "        corr_matrix = train_selected.corr().abs()\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        # Find index of feature columns with correlation greater than 0.8\n",
    "        to_drop = [column for column in upper.columns if any(upper[column] > 0.8)]\n",
    "        train_selected.drop(columns=to_drop, inplace=True)\n",
    "        test_selected.drop(columns=to_drop, inplace=True)\n",
    "\n",
    "        del to_drop\n",
    "\n",
    "    # Using top 1750 features\n",
    "    nextSel = SelectKBest(score_func=f_classif, k=numFeatures)\n",
    "    nextSel.fit_transform(train_selected, train_labels['label'])\n",
    "    cols = []\n",
    "    cols = [column for column in train_selected.columns if column not in train_selected.columns[nextSel.get_support()]]\n",
    "    train_selected.drop(columns=cols, inplace=True)\n",
    "    test_selected.drop(columns=cols, inplace=True)\n",
    "\n",
    "    print(f'Feature data shape after SelectKBest using f-value stats: {train_selected.shape}')\n",
    "\n",
    "    # Free up memory for next tasks\n",
    "    del toDrop, cols, sel, nextSel\n",
    "\n",
    "    return train_selected, test_selected"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Trees Model\n",
    "**Model**\n",
    "\n",
    "Using a Extremely Randomized Trees approach to classify samples, an ensemble learning method akin to Random Forests is used to determine class labels for examples. \n",
    "\n",
    "### Training\n",
    "\n",
    "**Hyperparameters** \n",
    "\n",
    "The following hyperparameters were determined experimentally using a grid search method \n",
    "- Number of trees in Forest: 1000\n",
    "- Loss Criterion: Log Loss\n",
    "- Input size: 1250\n",
    "- Output size: 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import time\n",
    "\n",
    "# Dataset Preparation\n",
    "dataset, evalSet = trimDataset(1250, False)\n",
    "dataset.sort_values(by=['sample_id'], inplace=True); evalSet.sort_values(by=['sample_id'], inplace=True)\n",
    "\n",
    "dataset['label'] = train_labels['label']\n",
    "dataset = dataset.sample(frac=1) # Shuffle rows before breaking into sets\n",
    "\n",
    "print(dataset.shape, ' --> Dataset shape prior to training')\n",
    "\n",
    "trainData, trainLabels, testData, testLabels = train_test_split(dataset, train_labels, shuffle=False, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model Training\n",
    "eTree = ExtraTreesClassifier(n_estimators=1000, min_samples_split=2, max_features=None, n_jobs=10, criterion='log_loss')\n",
    "\n",
    "start = time.time()\n",
    "eTree.fit(trainData, trainLabels)\n",
    "totalTime = time.time() - start\n",
    "print(f'ETrees Training time: {round(totalTime, ndigits=2)} s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "Testing model using 10% of the original dataset used as a holdout for validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Extra Trees\n",
    "trainLabelsEx = pd.Series(data=eTree.predict(X=trainData), index=trainData.index) # output trainData.shape\n",
    "testLabelsEx = pd.Series(data=eTree.predict(X=testData), index=testData.index)    # output testData.shape\n",
    "\n",
    "tr_score = accuracy_score(trainLabels, trainLabelsEx)\n",
    "te_score = accuracy_score(testLabels,  testLabelsEx)\n",
    "\n",
    "print(f\"Train accuracy ETrees: {round(tr_score*100, 2)}%\")\n",
    "print(f\"Test accuracy ETrees:  {round(te_score*100, 2)}%\")\n",
    "\n",
    "score = cross_val_score(eTree, X=testData, y=testLabels, cv=5, n_jobs=10, scoring='accuracy')\n",
    "print(\"5-fold Cross validation score on eTree model\")\n",
    "print(f\"Score: {score.mean(): .2f}\\t[Â± {score.std(): .2f}]\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset, evalSet = trimDataset(1750, False)\n",
    "trainX, testX, trainY, testY = train_test_split(dataset, train_labels, shuffle=True, test_size=0.2, random_state=42)\n",
    "\n",
    "from sklearn import svm\n",
    "svm_model = svm.SVC(kernel='linear', C = 1.0)\n",
    "\n",
    "svm_model.fit(trainX, trainY.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train_pred = svm_model.predict(trainX)\n",
    "test_pred = svm_model.predict(testX)\n",
    "\n",
    "print(\"====== Training dataset results ======\")\n",
    "print(f\"Training accuracy: {accuracy_score(trainY, train_pred)}\")\n",
    "print(\"====== Validation dataset results ======\")\n",
    "print(f\"Testing accuracy: {accuracy_score(testY, test_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores_2 = cross_val_score(svm_model, dataset, train_labels.values.ravel(), cv=5)\n",
    "print(\"%0.5f accuracy with a standard deviation of %0.5f\" % (scores_2.mean(), scores_2.std()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tr_split = 1300 # ~80% of labelled data\n",
    "\n",
    "# Dataset Preparation\n",
    "dataset, evalSet = trimDataset(1250, True)\n",
    "dataset.sort_values(by=['sample_id'], inplace=True); evalSet.sort_values(by=['sample_id'], inplace=True)\n",
    "\n",
    "dataset['label'] = train_labels['label']\n",
    "dataset = dataset.sample(frac=1) # Shuffle rows before breaking into sets\n",
    "\n",
    "print(dataset.shape, '- Dataset shape prior to training')\n",
    "\n",
    "trainSet = dataset.iloc[:tr_split] # Shape: 1400, numFeatures\n",
    "testSet = dataset.iloc[tr_split:]  # Shape: 221, numFeatures\n",
    "\n",
    "xgb_cl = xgb.XGBClassifier(n_estimators=1000, min_samples_split=2, max_features=None, n_jobs=10, criterion='log_loss')\n",
    "\n",
    "trainData = trainSet.iloc[:, :-1] # 1400, 1700\n",
    "trainLabels = trainSet.iloc[:,-1] # 1400, 1\n",
    "\n",
    "testData = testSet.iloc[:, :-1] # 1400, 1700\n",
    "testLabels = testSet.iloc[:,-1] # 1400, 1\n",
    "\n",
    "start = time.time()\n",
    "xgb_cl.fit(trainData, trainLabels)\n",
    "totalTime = time.time() - start\n",
    "print(f'XGBoost Training time: {round(totalTime, ndigits=2)} s')\n",
    "\n",
    "test_preds = xgb_cl.predict(testData)\n",
    "\n",
    "a_s1 = accuracy_score(testLabels, test_preds)\n",
    "\n",
    "print(a_s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores_2 = cross_val_score(xgb_cl, trainData, trainLabels, cv=5)\n",
    "print(\"%0.5f accuracy with a standard deviation of %0.5f\" % (scores_2.mean(), scores_2.std()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
