{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "y_Ivqbm7YqoC"
      },
      "outputs": [],
      "source": [
        "import os, sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXLytjSsYdSK",
        "outputId": "e3197da5-8952-4223-cf85-c0bb77d7f3b9"
      },
      "outputs": [],
      "source": [
        "# Check if extracted data folder exists\n",
        "usingColab = 0\n",
        "if usingColab:\n",
        "  if(not os.path.exists('SYSC4415W23_A3_dataset')):\n",
        "\n",
        "    # Download and extract the dataset if the zip file does not exist\n",
        "    if (not os.path.isfile('SYSC4415W23_A3_dataset.zip')):\n",
        "      !wget https://github.com/jrgreen7/SYSC4906/releases/download/Assignment3/SYSC4415W23_A3_dataset.zip\n",
        "      !unzip SYSC4415W23_A3_dataset.zip\n",
        "\n",
        "  datasetPath = r\"SYSC4415W23_A3_dataset\"\n",
        "\n",
        "else:\n",
        "  datasetPath = r\"C:/Users/googl/Documents/ML/A3_Dataset/SYSC4415W23_A3_dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d63-SPPIXMyk",
        "outputId": "d6892d41-7e95-4b10-bad3-67a55375efa6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of original features: (1621, 7047)\n"
          ]
        }
      ],
      "source": [
        "train_labels = pd.read_csv(f'{datasetPath}/train/labels.csv')\n",
        "train_features = pd.read_csv(f'{datasetPath}/train/extracted_features.csv')\n",
        "\n",
        "train_features.set_index(keys='sample_id', inplace=True)\n",
        "train_features.sort_values(by=['sample_id'], inplace=True)\n",
        "train_labels.set_index(keys='sample_id', inplace=True)\n",
        "train_labels.sort_values(by=['sample_id'], inplace=True)\n",
        "\n",
        "print(f'Shape of original features: {train_features.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data visualization\n",
        "counts = train_labels.value_counts(subset=['label'])\n",
        "keys = ['Normal Walk', 'Fast Walk', 'Ascent', 'Descent', 'Jumping Jacks']\n",
        "vals = [counts[0],counts[1], counts[2], counts[3], counts[4]]\n",
        "\n",
        "fig = plt.figure(figsize=(10,5))\n",
        "plt.bar(x=keys, height=vals, width=0.5)\n",
        "plt.xlabel(\"Data Label\")\n",
        "plt.ylabel(\"Num samples\")\n",
        "plt.title(\"Number of each type of training sample\")\n",
        "plt.show()\n",
        "\n",
        "# No class imbalance"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9E1ch3OR3JHV"
      },
      "source": [
        "### Feature Selection \n",
        "Using variance thresholding, dropping any features with NaN values and using univariate stats to determince the ***__top 50% of features__*** for classification to determine which features are useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcFYbNc73Frh",
        "outputId": "fac3f48e-03c2-48eb-a23b-63993c9fdc88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of reduced features from removing NaN values: (1621, 6993)\n",
            "Shape after dropping columns with variance lower than .9525: (1621, 5402)\n",
            "Feature data shape after SelectKBest using f-value stats: (1621, 2000)\n"
          ]
        }
      ],
      "source": [
        "# Remove features that have very low variance\n",
        "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif\n",
        "\n",
        "# Drop columns with nan values\n",
        "toDrop = train_features.columns[train_features.isnull().any()].tolist()\n",
        "train_selected = train_features.drop(toDrop, axis=1)\n",
        "print(f'Number of reduced features from removing NaN values: {train_selected.shape}')\n",
        "\n",
        "# Drop columns with very low variance\n",
        "sel = VarianceThreshold(threshold=(0.95 * (1 - 0.95)))\n",
        "sel.fit_transform(train_selected)\n",
        "cols = [column for column in train_selected.columns \n",
        "          if column not in train_selected.columns[sel.get_support()]]\n",
        "train_selected.drop(columns=cols, inplace=True)\n",
        "print(f'Shape after dropping columns with variance lower than .9525: {train_selected.shape}')\n",
        "\n",
        "# # Find indexes of columns with correlation greater than 0.8\n",
        "# corr_matrix = train_selected.corr().abs()\n",
        "# upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "# to_drop = []\n",
        "\n",
        "# to_drop = [column for column in upper.columns if any(upper[column] > 0.8)]\n",
        "# train_selected.drop(columns=to_drop, inplace=True)\n",
        "# print(f'Number of features after removing columns with high correlation: {train_selected.shape}')\n",
        "\n",
        "# Using top 1750 features\n",
        "numFeatures = 2000\n",
        "nextSel = SelectKBest(score_func=f_classif, k=numFeatures)\n",
        "nextSel.fit_transform(train_selected, train_labels['label'])\n",
        "cols = [column for column in train_selected.columns if column not in train_selected.columns[nextSel.get_support()]]\n",
        "dataset = train_selected.drop(columns=cols)\n",
        "print(f'Feature data shape after SelectKBest using f-value stats: {dataset.shape}')\n",
        "\n",
        "# Free up memory for next tasks\n",
        "del toDrop, cols, sel, nextSel\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDFlUveodaF2"
      },
      "source": [
        "#### Data Loading and organization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "DSG1EIZmdaF3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1621, 2001) - Dataset shape prior to training\n"
          ]
        }
      ],
      "source": [
        "# from torch.utils.data import DataLoader\n",
        "batchSize = 32\n",
        "tr_split = 1300 # ~80% of labelled data\n",
        "te_split = 321  # ~10% of labelled data\n",
        "\n",
        "dataset['label'] = train_labels['label']\n",
        "dataset = dataset.sample(frac=1) # Shuffle rows before breaking into sets\n",
        "\n",
        "print(dataset.shape, '- Dataset shape prior to training')\n",
        "\n",
        "trainSet = dataset.iloc[:tr_split] # Shape: 1400, numFeatures\n",
        "testSet = dataset.iloc[tr_split:]  # Shape: 221, numFeatures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4JxEwwBdaF4"
      },
      "source": [
        "### Building model\n",
        "**Hyperparameters**: \n",
        "- Learning rate: Using effective LR calculated from base LR using:\n",
        "$$\\eta_{eff} = \\frac{B\\eta_{base}}{256}$$\n",
        "- Input size: 2700\n",
        "- Output size: 5 (1 per class label)\n",
        "- Hiddel layers: 1\n",
        "- Nodes/hidden layer: 1800 (2/3 of input size, since input size > num samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtoXtPV_daF4",
        "outputId": "b3a215d9-507f-4eca-ed51-0849741f5889"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "import time\n",
        "lr_base = 0.0001\n",
        "maxIterations = 1500\n",
        "\n",
        "lr = (batchSize*lr_base)/256\n",
        "MLP = MLPClassifier((1000,), 'relu', solver='adam', learning_rate_init=lr, max_iter=maxIterations, batch_size=batchSize, tol=0.00001)\n",
        "trainData = trainSet.iloc[:, :-1] # 1300, 1500\n",
        "trainLabels = trainSet.iloc[:,-1] # 1300, 1\n",
        "\n",
        "start = time.time()\n",
        "# MLP.fit(trainData, trainLabels)\n",
        "totalTime = time.time() - start\n",
        "print(f'Training time: {round(totalTime, ndigits=2)} s')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# losses = MLP.loss_curve_\n",
        "# valScores = MLP.validation_scores_\n",
        "# iterations = np.linspace(0, MLP.n_iter_, len(losses))\n",
        "\n",
        "# print(f'Number of iterations: {MLP.n_iter_}')\n",
        "# figs, ax = plt.subplots(nrows=1, ncols=1)\n",
        "# ax.plot(iterations, losses)\n",
        "# ax.set_title('Training losses'); ax.set_xlabel('Iteration'); ax.set_ylabel('Loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ETrees Training time: 24.76 s\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "import sklearn.ensemble as en\n",
        "import time\n",
        "\n",
        "eTree = ExtraTreesClassifier(n_estimators=900, min_samples_split=2, max_features=None, n_jobs=10, criterion='entropy')\n",
        "\n",
        "trainData = trainSet.iloc[:, :-1] # 1400, 1700\n",
        "trainLabels = trainSet.iloc[:,-1] # 1400, 1\n",
        "\n",
        "start = time.time()\n",
        "eTree.fit(trainData, trainLabels)\n",
        "totalTime = time.time() - start\n",
        "print(f'ETrees Training time: {round(totalTime, ndigits=2)} s')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZL0zKxELGx_",
        "outputId": "2cf47384-fb71-4807-e516-c3f158018b36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train accuracy ERT: 100.0%\n",
            "Test accuracy ERT: 77.26%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "testCols = testSet.columns\n",
        "for col in trainSet.columns:\n",
        "  if col not in testCols:\n",
        "    testSet.drop(columns=col)\n",
        "\n",
        "testData = testSet.iloc[:, :-1] # 321, 1500\n",
        "testLabels = testSet.iloc[:,-1] # 321, 1\n",
        "\n",
        "# Extra Trees\n",
        "trainLabelsEx_ert = pd.Series(data=eTree.predict(X=trainData), index=trainData.index) # output 1400, 1\n",
        "testLabelsEx_ert = pd.Series(data=eTree.predict(X=testData), index=testData.index)    # output 221, 1\n",
        "\n",
        "tr_score = accuracy_score(trainLabels, trainLabelsEx_ert)\n",
        "te_score = accuracy_score(testLabels, testLabelsEx_ert)\n",
        "\n",
        "print(f\"Train accuracy ERT: {round(tr_score*100, 2)}%\")\n",
        "print(f\"Test accuracy ERT: {round(te_score*100, 2)}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5-fold Cross validation score on eTree model\n",
            "Score:  0.63\t[±  0.08]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "score = cross_val_score(eTree, X=testData, y=testLabels, cv=5, n_jobs=10, scoring='accuracy')\n",
        "print(\"5-fold Cross validation score on eTree model\")\n",
        "print(f\"Score: {score.mean(): .2f}\\t[± {score.std(): .2f}]\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ML",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "44ebdb7eb0a19e41afd994342a776f6179e5d196f7f1f7cdbb3f2d4517dc929d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
