{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_Ivqbm7YqoC"
      },
      "outputs": [],
      "source": [
        "import os, sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXLytjSsYdSK",
        "outputId": "e3197da5-8952-4223-cf85-c0bb77d7f3b9"
      },
      "outputs": [],
      "source": [
        "# Check if extracted data folder exists\n",
        "if(not os.path.exists('SYSC4415W23_A3_dataset')):\n",
        "\n",
        "  # Download and extract the dataset if the zip file does not exist\n",
        "  if (not os.path.isfile('SYSC4415W23_A3_dataset.zip')):\n",
        "    !wget https://github.com/jrgreen7/SYSC4906/releases/download/Assignment3/SYSC4415W23_A3_dataset.zip\n",
        "    !unzip SYSC4415W23_A3_dataset.zip\n",
        "\n",
        "datasetPath = r\"SYSC4415W23_A3_dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d63-SPPIXMyk",
        "outputId": "d6892d41-7e95-4b10-bad3-67a55375efa6"
      },
      "outputs": [],
      "source": [
        "train_labels = pd.read_csv(f'{datasetPath}/train/labels.csv')\n",
        "train_features = pd.read_csv(f'{datasetPath}/train/extracted_features.csv')\n",
        "\n",
        "train_features.set_index(keys='sample_id', inplace=True)\n",
        "train_features.sort_values(by=['sample_id'], inplace=True)\n",
        "train_labels.set_index(keys='sample_id', inplace=True)\n",
        "train_labels.sort_values(by=['sample_id'], inplace=True)\n",
        "\n",
        "print(f'Shape of original features: {train_features.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data visualization\n",
        "counts = train_labels.value_counts(subset=['label'])\n",
        "keys = ['Normal Walk', 'Fast Walk', 'Ascent', 'Descent', 'Jumping Jacks']\n",
        "vals = [counts[0],counts[1], counts[2], counts[3], counts[4]]\n",
        "\n",
        "fig = plt.figure(figsize=(10,5))\n",
        "plt.bar(x=keys, height=vals, width=0.5)\n",
        "plt.xlabel(\"Data Label\")\n",
        "plt.ylabel(\"Num samples\")\n",
        "plt.title(\"Number of each type of training sample\")\n",
        "plt.show()\n",
        "\n",
        "# No class imbalance"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9E1ch3OR3JHV"
      },
      "source": [
        "### Feature Selection \n",
        "Using variance thresholding, dropping any features with NaN values and using univariate stats to determince the ***__top 50% of features__*** for classification to determine which features are useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcFYbNc73Frh",
        "outputId": "fac3f48e-03c2-48eb-a23b-63993c9fdc88"
      },
      "outputs": [],
      "source": [
        "# Remove features that have very low variance\n",
        "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif\n",
        "\n",
        "# Drop columns with nan values\n",
        "toDrop = train_features.columns[train_features.isnull().any()].tolist()\n",
        "train_selected = train_features.drop(toDrop, axis=1)\n",
        "print(f'Number of reduced features from removing NaN values: {train_selected.shape}')\n",
        "\n",
        "# Drop columns with very low variance\n",
        "sel = VarianceThreshold(threshold=(0.95 * (1 - 0.95)))\n",
        "sel.fit_transform(train_selected)\n",
        "cols = [column for column in train_selected.columns \n",
        "          if column not in train_selected.columns[sel.get_support()]]\n",
        "train_selected.drop(columns=cols, inplace=True)\n",
        "print(f'Shape after dropping columns with variance lower than .9525: {train_selected.shape}')\n",
        "\n",
        "# Using top k features\n",
        "numFeatures = 1250\n",
        "nextSel = SelectKBest(score_func=f_classif, k=numFeatures)\n",
        "nextSel.fit_transform(train_selected, train_labels['label'])\n",
        "cols = [column for column in train_selected.columns if column not in train_selected.columns[nextSel.get_support()]]\n",
        "dataset = train_selected.drop(columns=cols)\n",
        "print(f'Feature data shape after SelectKBest using f-value stats: {dataset.shape}')\n",
        "\n",
        "# Free up memory for next tasks\n",
        "del toDrop, cols, sel, nextSel\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDFlUveodaF2"
      },
      "source": [
        "#### Data Loading and organization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSG1EIZmdaF3"
      },
      "outputs": [],
      "source": [
        "# from torch.utils.data import DataLoader\n",
        "tr_split = 1300 # ~80% of labelled data\n",
        "\n",
        "dataset['label'] = train_labels['label']\n",
        "dataset = dataset.sample(frac=1) # Shuffle rows before breaking into sets\n",
        "\n",
        "print(dataset.shape, '- Dataset shape prior to training')\n",
        "\n",
        "trainSet = dataset.iloc[:tr_split] # Shape: 1400, numFeatures\n",
        "testSet = dataset.iloc[tr_split:]  # Shape: 221, numFeatures"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "a4JxEwwBdaF4"
      },
      "source": [
        "### Building model\n",
        "**Model**\n",
        "\n",
        "Using a Extremely Randomized Trees approach to classify samples, an ensemble learning method akin to Random Forests is used to determine class labels for examples. \n",
        "\n",
        "**Hyperparameters** \n",
        "- Number of trees in Forest: 1000\n",
        "- Input size: 1250\n",
        "- Output size: 5 (1 per class label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "import time\n",
        "\n",
        "eTree = ExtraTreesClassifier(n_estimators=1000, min_samples_split=2, max_features=None, n_jobs=10, criterion='log_loss')\n",
        "\n",
        "trainData = trainSet.iloc[:, :-1] # 1400, 1700\n",
        "trainLabels = trainSet.iloc[:,-1] # 1400, 1\n",
        "\n",
        "start = time.time()\n",
        "eTree.fit(trainData, trainLabels)\n",
        "totalTime = time.time() - start\n",
        "print(f'ETrees Training time: {round(totalTime, ndigits=2)} s')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZL0zKxELGx_",
        "outputId": "2cf47384-fb71-4807-e516-c3f158018b36"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "testData = testSet.iloc[:, :-1] # 321, numFeatures\n",
        "testLabels = testSet.iloc[:,-1] # 321, 1\n",
        "\n",
        "# Extra Trees\n",
        "trainLabelsEx_ert = pd.Series(data=eTree.predict(X=trainData), index=trainData.index) # output 1400, 1\n",
        "testLabelsEx_ert = pd.Series(data=eTree.predict(X=testData), index=testData.index)    # output 221, 1\n",
        "\n",
        "tr_score = accuracy_score(trainLabels, trainLabelsEx_ert)\n",
        "te_score = accuracy_score(testLabels, testLabelsEx_ert)\n",
        "\n",
        "print(f\"Train accuracy ERT: {round(tr_score*100, 2)}%\")\n",
        "print(f\"Test accuracy ERT: {round(te_score*100, 2)}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "score = cross_val_score(eTree, X=testData, y=testLabels, cv=5, n_jobs=10, scoring='accuracy')\n",
        "print(\"5-fold Cross validation score on eTree model\")\n",
        "print(f\"Score: {score.mean(): .2f}\\t[Â± {score.std(): .2f}]\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ML",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "44ebdb7eb0a19e41afd994342a776f6179e5d196f7f1f7cdbb3f2d4517dc929d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
